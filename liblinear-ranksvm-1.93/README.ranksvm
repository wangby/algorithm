This extension of liblinear efficiently trains L2-regularized L2-loss linear
rankSVM by trust region Newton method and selection trees.
The implementation details can be found in the following paper:
Ching-Pei Lee and Chih-Jen Lin, Large-scale Linear RankSVM, 2013.
http://www.csie.ntu.edu.tw/~cjlin/papers/ranksvml2.pdf

Usage
=====

The usage is the same as liblinear except the following additional option:

-s 8 : L2-regularized L2-loss ranking support vector machine (primal)

Note that the default solver in this extension is -s 8.

The file format is also slightly different. In this tool we support multiple
queries in a training data by specifying a feature called qid.
The file format is the same as liblinear:
<label> <index1>:<value1> <index2>:<value2> ...
<label> can be any real number as regression, while <index> can be either the
string "qid" or an integer starting from 1. The use of qid is to specify
different queries. The preference pairs are formed by instances with the same
qid. Note that we only support either each instance has a qid or none of the
instance has a qid.
To support this different file format, a modified svm-scale is also included
in this tool. A sample ranking data included in this tool is
'bodyfat_scale_qid'.

Examples
========

	$ ./train -s 8 bodyfat_scale_qid

Difference to liblinear
=======================

This extension mainly includes two new files: ranksvm.h and ranksvm.cpp.

In these two files we implement a selection tree, a ranksvm solver using trust
region Newton method and provide two functions: eval_list for evaluating
ranking performance and rank_cross_validation for ranksvm cross validation.
We use the selection tree to compute the function value, gradient and
Hessian-vector products, which are needed by trust region Newton method.

The evaluation metrics considered are pairwise accuracy and the mean NDCG
formulation of LETOR data sets. For cross validation, we conduct data
splitting in query level, and when there is only one query, each fold is
treated as a separate query to avoid ranking inconsistency between different
models.
